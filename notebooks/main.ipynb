{"cells":[{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":8161,"status":"ok","timestamp":1710096772882,"user":{"displayName":"BAICHUAN WANG","userId":"06992025179668429938"},"user_tz":420},"id":"8POg3lKjABRo"},"outputs":[{"name":"stdout","output_type":"stream","text":["Using cpu.\n"]}],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","from sklearn.model_selection import train_test_split\n","\n","import sys\n","import os\n","\n","os.chdir('..')\n","sys.path.append(os.getcwd())\n","\n","from models.cnn_lstm import *\n","from scripts.preprocessing import *"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10428,"status":"ok","timestamp":1710096783298,"user":{"displayName":"BAICHUAN WANG","userId":"06992025179668429938"},"user_tz":420},"id":"IdqLdY88ABRp","outputId":"504f9ea4-73f6-4e10-c321-d646dc4c2442"},"outputs":[],"source":["# Load Data\n","os.chdir(os.getcwd() + r'\\project_data\\project')\n","X_test = np.load(\"X_test.npy\")\n","y_test = np.load(\"y_test.npy\")\n","person_train_valid = np.load(\"person_train_valid.npy\")\n","X_train_valid = np.load(\"X_train_valid.npy\")\n","y_train_valid = np.load(\"y_train_valid.npy\")\n","person_test = np.load(\"person_test.npy\")\n","\n","## Adjusting the labels so that\n","\n","# Cue onset left - 0\n","# Cue onset right - 1\n","# Cue onset foot - 2\n","# Cue onset tongue - 3\n","\n","y_train_valid -= 769\n","y_test -= 769"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1710096783298,"user":{"displayName":"BAICHUAN WANG","userId":"06992025179668429938"},"user_tz":420},"id":"PV26nQ6RNjMh"},"outputs":[],"source":["## Data Augumentation\n","def aug(X_train, y_train, X_test, y_test, Trim, sub_sample, average, noise, noid):\n","\n","    total_X_train = None\n","    total_y_train = None\n","    total_X_test = None\n","    total_y_test = None\n","\n","    #Trimming\n","    H = int(Trim * X_train.shape[2])\n","\n","    X_train = X_train[:, :, 0:H]\n","\n","    X_test = X_test[:,:, 0:H]\n","\n","    #Maxpooling\n","    X_max_train = np.max(X_train.reshape(X_train.shape[0], X_train.shape[1], -1, sub_sample), axis=3)\n","\n","    X_max_test = np.max(X_test.reshape(X_test.shape[0], X_test.shape[1], -1, sub_sample), axis=3)\n","\n","    total_X_train = X_max_train\n","    total_y_train = y_train\n","\n","    total_X_test = X_max_test\n","    total_y_test = y_test\n","\n","    #Jittering\n","    X_average_train = np.mean(X_train.reshape(X_train.shape[0], X_train.shape[1], -1, average),axis=3)\n","    X_average_test = np.mean(X_test.reshape(X_test.shape[0], X_test.shape[1], -1, average),axis=3)\n","\n","    X_average_train = X_average_train + np.random.normal(0.0, noise, X_average_train.shape)\n","    X_average_test = X_average_test + np.random.normal(0.0, noise, X_average_test.shape)\n","\n","    total_X_train = np.vstack((total_X_train, X_average_train))\n","    total_y_train = np.hstack((total_y_train, y_train))\n","\n","    total_X_test = np.vstack((total_X_test, X_average_test))\n","    total_y_test = np.hstack((total_y_test, y_test))\n","\n","    #Subsampling\n","    for i in range(sub_sample):\n","\n","        X_subsample_train = X_train[:, :, i::sub_sample] + \\\n","                            (np.random.normal(0.0, noise, X_train[:, :,i::sub_sample].shape) if noid else 0.0)\n","\n","        total_X_train = np.vstack((total_X_train, X_subsample_train))\n","        total_y_train = np.hstack((total_y_train, y_train))\n","\n","        X_subsample_test = X_test[:, :, i::sub_sample] + \\\n","                            (np.random.normal(0.0, noise, X_test[:, :,i::sub_sample].shape) if noid else 0.0)\n","\n","        total_X_test = np.vstack((total_X_test, X_subsample_test))\n","        total_y_test = np.hstack((total_y_test, y_test))\n","\n","\n","    return total_X_train,total_y_train,total_X_test,total_y_test"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7982,"status":"ok","timestamp":1710096791278,"user":{"displayName":"BAICHUAN WANG","userId":"06992025179668429938"},"user_tz":420},"id":"eGMiFuK9Njx_","outputId":"40fddc59-db1a-4ad8-9eed-acd74f52d927"},"outputs":[{"name":"stdout","output_type":"stream","text":["(6768, 22, 500)\n","(1692, 22, 500)\n","(1772, 22, 500)\n","torch.Size([6768, 1, 22, 500])\n","torch.Size([1692, 1, 22, 500])\n","torch.Size([1772, 1, 22, 500])\n"]}],"source":["##Preprocessing\n","# Augmentation\n","X_train_valid_prep, y_train_valid_prep, X_test_prep, y_test_prep = aug(X_train_valid,y_train_valid,X_test,y_test,700,2,2,0.5,1)\n","# Splitting data\n","X_train_prep, X_valid_prep, y_train_prep, y_valid_prep = train_test_split(X_train_valid_prep, y_train_valid_prep, test_size=0.2, shuffle=True)\n","\n","print((X_train_prep).shape)\n","print((X_valid_prep).shape)\n","print((X_test_prep).shape)\n","\n","class EEGDataset(Dataset):\n","    def __init__(self, X, Y):\n","        self.X = torch.FloatTensor(X).unsqueeze(1)\n","        # self.X = self.X.permute(0, 3, 2, 1)\n","        self.Y = torch.LongTensor(Y)\n","\n","    def __len__(self):\n","        return len(self.X)\n","\n","    def __getitem__(self, index):\n","        return self.X[index], self.Y[index]\n","\n","    def shape(self):\n","        return self.X.shape\n","\n","# Creating Dataset instances\n","train_set = EEGDataset(X_train_prep, y_train_prep)\n","val_set = EEGDataset(X_valid_prep, y_valid_prep)\n","test_set = EEGDataset(X_test_prep, y_test_prep)\n","\n","print(train_set.shape())\n","print(val_set.shape())\n","print(test_set.shape())\n","\n","# Initializing DataLoaders with concise batch_size and shuffle parameters\n","train_loader = DataLoader(train_set, batch_size=100, shuffle=True)\n","val_loader = DataLoader(val_set, batch_size=100, shuffle=True)\n","test_loader = DataLoader(test_set, batch_size=50, shuffle=True)"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1710096791279,"user":{"displayName":"BAICHUAN WANG","userId":"06992025179668429938"},"user_tz":420},"id":"aWOrrQs8OK4b","outputId":"a50d6db0-7502-4a3a-a17c-d680c5e01b23"},"outputs":[],"source":["model = CNNLSTM()\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.005)"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"6LSQ6AosOWPQ"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/1, Training Loss: 1.3722237892010634, Validation Loss: 1.3281154071583468, Validation Accuracy: 40.780141843971634%\n"]}],"source":["num_epochs = 1\n","\n","for epoch in range(num_epochs):\n","    model.train()\n","    running_loss = 0.0\n","    for inputs, labels in train_loader:\n","        optimizer.zero_grad()\n","        outputs = model(inputs)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","        running_loss += loss.item()\n","\n","    model.eval()\n","    val_loss = 0.0\n","    correct = 0\n","    total = 0\n","    with torch.no_grad():\n","        for inputs, labels in val_loader:\n","            outputs = model(inputs)\n","            loss = criterion(outputs, labels)\n","            val_loss += loss.item()\n","            _, predicted = torch.max(outputs.data, 1)\n","            total += labels.size(0)\n","            correct += (predicted == labels).sum().item()\n","\n","    print(f\"Epoch {epoch+1}/{num_epochs}, Training Loss: {running_loss/len(train_loader)}, Validation Loss: {val_loss/len(val_loader)}, Validation Accuracy: {100 * correct / total}%\")\n"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Test Loss: 1.3851914770073361, Test Accuracy: 28.386004514672685%\n"]}],"source":["model.eval()\n","test_loss = 0.0\n","correct = 0\n","total = 0\n","with torch.no_grad():\n","    for inputs, labels in test_loader:\n","        outputs = model(inputs)\n","        loss = criterion(outputs, labels)\n","        test_loss += loss.item()\n","        _, predicted = torch.max(outputs.data, 1)\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item()\n","\n","print(f\"Test Loss: {test_loss/len(test_loader)}, Test Accuracy: {100 * correct / total}%\")"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"c147","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.9"}},"nbformat":4,"nbformat_minor":0}
