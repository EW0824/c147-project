{"cells":[{"cell_type":"code","source":["# For Google Colab\n","from google.colab import drive\n","drive.mount('/content/drive')\n","import sys\n","import os\n","sys.path.append('/content/drive/My Drive/c147-project')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_Tkq0x8vDr9j","executionInfo":{"status":"ok","timestamp":1710096764722,"user_tz":420,"elapsed":1290,"user":{"displayName":"BAICHUAN WANG","userId":"06992025179668429938"}},"outputId":"fedab211-cc12-40f9-99c7-66ff640658fc"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","execution_count":2,"metadata":{"id":"8POg3lKjABRo","executionInfo":{"status":"ok","timestamp":1710096772882,"user_tz":420,"elapsed":8161,"user":{"displayName":"BAICHUAN WANG","userId":"06992025179668429938"}}},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","from sklearn.model_selection import train_test_split\n","\n","from models.cnn_lstm import *\n","from scripts.preprocessing import *"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IdqLdY88ABRp","executionInfo":{"status":"ok","timestamp":1710096783298,"user_tz":420,"elapsed":10428,"user":{"displayName":"BAICHUAN WANG","userId":"06992025179668429938"}},"outputId":"504f9ea4-73f6-4e10-c321-d646dc4c2442"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/My Drive/c147-project/project_data/project\n"]}],"source":["# Load Data\n","%cd /content/drive/My Drive/c147-project/project_data/project\n","X_test = np.load(\"X_test.npy\")\n","y_test = np.load(\"y_test.npy\")\n","person_train_valid = np.load(\"person_train_valid.npy\")\n","X_train_valid = np.load(\"X_train_valid.npy\")\n","y_train_valid = np.load(\"y_train_valid.npy\")\n","person_test = np.load(\"person_test.npy\")\n","\n","## Adjusting the labels so that\n","\n","# Cue onset left - 0\n","# Cue onset right - 1\n","# Cue onset foot - 2\n","# Cue onset tongue - 3\n","\n","y_train_valid -= 769\n","y_test -= 769"]},{"cell_type":"code","source":["## Data Augumentation\n","def aug(X_train, y_train, X_test, y_test, Trim, sub_sample, average, noise, noid):\n","\n","    total_X_train = None\n","    total_y_train = None\n","    total_X_test = None\n","    total_y_test = None\n","\n","    #Trimming\n","    H = int(Trim * X_train.shape[2])\n","\n","    X_train = X_train[:, :, 0:H]\n","\n","    X_test = X_test[:,:, 0:H]\n","\n","    #Maxpooling\n","    X_max_train = np.max(X_train.reshape(X_train.shape[0], X_train.shape[1], -1, sub_sample), axis=3)\n","\n","    X_max_test = np.max(X_test.reshape(X_test.shape[0], X_test.shape[1], -1, sub_sample), axis=3)\n","\n","    total_X_train = X_max_train\n","    total_y_train = y_train\n","\n","    total_X_test = X_max_test\n","    total_y_test = y_test\n","\n","    #Jittering\n","    X_average_train = np.mean(X_train.reshape(X_train.shape[0], X_train.shape[1], -1, average),axis=3)\n","    X_average_test = np.mean(X_test.reshape(X_test.shape[0], X_test.shape[1], -1, average),axis=3)\n","\n","    X_average_train = X_average_train + np.random.normal(0.0, noise, X_average_train.shape)\n","    X_average_test = X_average_test + np.random.normal(0.0, noise, X_average_test.shape)\n","\n","    total_X_train = np.vstack((total_X_train, X_average_train))\n","    total_y_train = np.hstack((total_y_train, y_train))\n","\n","    total_X_test = np.vstack((total_X_test, X_average_test))\n","    total_y_test = np.hstack((total_y_test, y_test))\n","\n","    #Subsampling\n","    for i in range(sub_sample):\n","\n","        X_subsample_train = X_train[:, :, i::sub_sample] + \\\n","                            (np.random.normal(0.0, noise, X_train[:, :,i::sub_sample].shape) if noid else 0.0)\n","\n","        total_X_train = np.vstack((total_X_train, X_subsample_train))\n","        total_y_train = np.hstack((total_y_train, y_train))\n","\n","        X_subsample_test = X_test[:, :, i::sub_sample] + \\\n","                            (np.random.normal(0.0, noise, X_test[:, :,i::sub_sample].shape) if noid else 0.0)\n","\n","        total_X_test = np.vstack((total_X_test, X_subsample_test))\n","        total_y_test = np.hstack((total_y_test, y_test))\n","\n","\n","    return total_X_train,total_y_train,total_X_test,total_y_test"],"metadata":{"id":"PV26nQ6RNjMh","executionInfo":{"status":"ok","timestamp":1710096783298,"user_tz":420,"elapsed":3,"user":{"displayName":"BAICHUAN WANG","userId":"06992025179668429938"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["##Preprocessing\n","# Augmentation\n","X_train_valid_prep, y_train_valid_prep, X_test_prep, y_test_prep = aug(X_train_valid,y_train_valid,X_test,y_test,700,2,2,0.5,1)\n","# Splitting data\n","X_train_prep, X_valid_prep, y_train_prep, y_valid_prep = train_test_split(X_train_valid_prep, y_train_valid_prep, test_size=0.2, shuffle=True)\n","\n","print((X_train_prep).shape)\n","print((X_valid_prep).shape)\n","print((X_test_prep).shape)\n","\n","class EEGDataset(Dataset):\n","    def __init__(self, X, Y):\n","        self.X = torch.FloatTensor(X).unsqueeze(1)\n","        # self.X = self.X.permute(0, 3, 2, 1)\n","        self.Y = torch.LongTensor(Y)\n","\n","    def __len__(self):\n","        return len(self.X)\n","\n","    def __getitem__(self, index):\n","        return self.X[index], self.Y[index]\n","\n","    def shape(self):\n","        return self.X.shape\n","\n","# Creating Dataset instances\n","train_set = EEGDataset(X_train_prep, y_train_prep)\n","val_set = EEGDataset(X_valid_prep, y_valid_prep)\n","test_set = EEGDataset(X_test_prep, y_test_prep)\n","\n","print(train_set.shape())\n","print(val_set.shape())\n","print(test_set.shape())\n","\n","# Initializing DataLoaders with concise batch_size and shuffle parameters\n","train_loader = DataLoader(train_set, batch_size=100, shuffle=True)\n","val_loader = DataLoader(val_set, batch_size=100, shuffle=True)\n","test_loader = DataLoader(test_set, batch_size=50, shuffle=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eGMiFuK9Njx_","executionInfo":{"status":"ok","timestamp":1710096791278,"user_tz":420,"elapsed":7982,"user":{"displayName":"BAICHUAN WANG","userId":"06992025179668429938"}},"outputId":"40fddc59-db1a-4ad8-9eed-acd74f52d927"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["(6768, 22, 500)\n","(1692, 22, 500)\n","(1772, 22, 500)\n","torch.Size([6768, 1, 22, 500])\n","torch.Size([1692, 1, 22, 500])\n","torch.Size([1772, 1, 22, 500])\n"]}]},{"cell_type":"code","source":["model = CNNLSTM()\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aWOrrQs8OK4b","executionInfo":{"status":"ok","timestamp":1710096791279,"user_tz":420,"elapsed":8,"user":{"displayName":"BAICHUAN WANG","userId":"06992025179668429938"}},"outputId":"a50d6db0-7502-4a3a-a17c-d680c5e01b23"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.4 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n"]}]},{"cell_type":"code","source":["num_epochs = 100\n","\n","for epoch in range(num_epochs):\n","    model.train()\n","    running_loss = 0.0\n","    for inputs, labels in train_loader:\n","        optimizer.zero_grad()\n","        outputs = model(inputs)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","        running_loss += loss.item()\n","\n","    model.eval()\n","    val_loss = 0.0\n","    correct = 0\n","    total = 0\n","    with torch.no_grad():\n","        for inputs, labels in val_loader:\n","            outputs = model(inputs)\n","            loss = criterion(outputs, labels)\n","            val_loss += loss.item()\n","            _, predicted = torch.max(outputs.data, 1)\n","            total += labels.size(0)\n","            correct += (predicted == labels).sum().item()\n","\n","    print(f\"Epoch {epoch+1}/{num_epochs}, Training Loss: {running_loss/len(train_loader)}, Validation Loss: {val_loss/len(val_loader)}, Validation Accuracy: {100 * correct / total}%\")\n"],"metadata":{"id":"6LSQ6AosOWPQ"},"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"c147","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.9.undefined"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}