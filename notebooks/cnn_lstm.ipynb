{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1120,"status":"ok","timestamp":1710457676350,"user":{"displayName":"BAICHUAN WANG","userId":"06992025179668429938"},"user_tz":420},"id":"_Tkq0x8vDr9j","outputId":"af735eea-9711-4fef-f31a-9aff831d12d1"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["# For Google Colab\n","from google.colab import drive\n","drive.mount('/content/drive')\n","import sys\n","import os\n","sys.path.append('/content/drive/My Drive/c147-project')"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"8POg3lKjABRo","executionInfo":{"status":"ok","timestamp":1710457679937,"user_tz":420,"elapsed":3589,"user":{"displayName":"BAICHUAN WANG","userId":"06992025179668429938"}}},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","from sklearn.model_selection import train_test_split\n","\n","from models.cnn_lstm import *\n","from scripts.preprocessing import *\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":401,"status":"ok","timestamp":1710457680329,"user":{"displayName":"BAICHUAN WANG","userId":"06992025179668429938"},"user_tz":420},"id":"IdqLdY88ABRp","outputId":"1dd5afc5-7e66-4bca-a438-4262ab857d00"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/My Drive/c147-project/project_data/project\n"]}],"source":["# Load Data\n","%cd /content/drive/My Drive/c147-project/project_data/project\n","X_test = np.load(\"X_test.npy\")\n","y_test = np.load(\"y_test.npy\")\n","person_train_valid = np.load(\"person_train_valid.npy\")\n","X_train_valid = np.load(\"X_train_valid.npy\")\n","y_train_valid = np.load(\"y_train_valid.npy\")\n","person_test = np.load(\"person_test.npy\")\n","\n","## Adjusting the labels so that\n","\n","# Cue onset left - 0\n","# Cue onset right - 1\n","# Cue onset foot - 2\n","# Cue onset tongue - 3\n","\n","y_train_valid -= 769\n","y_test -= 769"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"PV26nQ6RNjMh","executionInfo":{"status":"ok","timestamp":1710457680667,"user_tz":420,"elapsed":339,"user":{"displayName":"BAICHUAN WANG","userId":"06992025179668429938"}}},"outputs":[],"source":["## Data Augumentation\n","def aug(X_train, y_train, X_test, y_test, Trim, sub_sample, average, noise, noid):\n","\n","    total_X_train = None\n","    total_y_train = None\n","    total_X_test = None\n","    total_y_test = None\n","\n","    #Trimming\n","    H = int(Trim * X_train.shape[2])\n","\n","    X_train = X_train[:, :, 0:H]\n","\n","    X_test = X_test[:,:, 0:H]\n","\n","    #Maxpooling\n","    X_max_train = np.max(X_train.reshape(X_train.shape[0], X_train.shape[1], -1, sub_sample), axis=3)\n","\n","    X_max_test = np.max(X_test.reshape(X_test.shape[0], X_test.shape[1], -1, sub_sample), axis=3)\n","\n","    total_X_train = X_max_train\n","    total_y_train = y_train\n","\n","    total_X_test = X_max_test\n","    total_y_test = y_test\n","\n","    #Jittering\n","    X_average_train = np.mean(X_train.reshape(X_train.shape[0], X_train.shape[1], -1, average),axis=3)\n","    X_average_test = np.mean(X_test.reshape(X_test.shape[0], X_test.shape[1], -1, average),axis=3)\n","\n","    X_average_train = X_average_train + np.random.normal(0.0, noise, X_average_train.shape)\n","    X_average_test = X_average_test + np.random.normal(0.0, noise, X_average_test.shape)\n","\n","    total_X_train = np.vstack((total_X_train, X_average_train))\n","    total_y_train = np.hstack((total_y_train, y_train))\n","\n","    total_X_test = np.vstack((total_X_test, X_average_test))\n","    total_y_test = np.hstack((total_y_test, y_test))\n","\n","    #Subsampling\n","    for i in range(sub_sample):\n","\n","        X_subsample_train = X_train[:, :, i::sub_sample] + \\\n","                            (np.random.normal(0.0, noise, X_train[:, :,i::sub_sample].shape) if noid else 0.0)\n","\n","        total_X_train = np.vstack((total_X_train, X_subsample_train))\n","        total_y_train = np.hstack((total_y_train, y_train))\n","\n","        X_subsample_test = X_test[:, :, i::sub_sample] + \\\n","                            (np.random.normal(0.0, noise, X_test[:, :,i::sub_sample].shape) if noid else 0.0)\n","\n","        total_X_test = np.vstack((total_X_test, X_subsample_test))\n","        total_y_test = np.hstack((total_y_test, y_test))\n","\n","\n","    return total_X_train,total_y_train,total_X_test,total_y_test\n","\n","\n","def aug_valid(X_train, y_train, Trim, sub_sample, average, noise, noid):\n","\n","    total_X_train = None\n","    total_y_train = None\n","\n","    #Trimming\n","    H = int(Trim * X_train.shape[2])\n","    X_train = X_train[:, :, 0:H]\n","\n","    #Maxpooling\n","    X_max_train = np.max(X_train.reshape(X_train.shape[0], X_train.shape[1], -1, sub_sample), axis=3)\n","\n","    total_X_train = X_max_train\n","    total_y_train = y_train\n","\n","    return total_X_train,total_y_train"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4245,"status":"ok","timestamp":1710457684910,"user":{"displayName":"BAICHUAN WANG","userId":"06992025179668429938"},"user_tz":420},"id":"eGMiFuK9Njx_","outputId":"5019497c-7966-4c45-9bc3-577e3107b980"},"outputs":[{"output_type":"stream","name":"stdout","text":["(6768, 22, 500)\n","(423, 22, 500)\n","(1772, 22, 500)\n","torch.Size([6768, 22, 500])\n","torch.Size([423, 22, 500])\n","torch.Size([1772, 22, 500])\n"]}],"source":["X_train, X_valid, y_train, y_valid = train_test_split(X_train_valid, y_train_valid, test_size=0.2, shuffle=True)\n","X_train_prep, y_train_prep, X_test_prep, y_test_prep = aug(X_train,y_train,X_test,y_test,700,2,2,0.5,1)\n","X_valid_prep, y_valid_prep = aug_valid(X_valid,y_valid,700,2,2,0.5,1)\n","\n","print((X_train_prep).shape)\n","print((X_valid_prep).shape)\n","print((X_test_prep).shape)\n","\n","class EEGDataset(Dataset):\n","    def __init__(self, X, Y):\n","        # self.X = torch.FloatTensor(X).unsqueeze(1)\n","        self.X = torch.FloatTensor(X) # for conv1d\n","        # self.X = self.X.permute(0, 3, 2, 1)\n","        self.Y = torch.LongTensor(Y)\n","\n","    def __len__(self):\n","        return len(self.X)\n","\n","    def __getitem__(self, index):\n","        return self.X[index], self.Y[index]\n","\n","    def shape(self):\n","        return self.X.shape\n","\n","# Creating Dataset instances\n","train_set = EEGDataset(X_train_prep, y_train_prep)\n","val_set = EEGDataset(X_valid_prep, y_valid_prep)\n","test_set = EEGDataset(X_test_prep, y_test_prep)\n","\n","print(train_set.shape())\n","print(val_set.shape())\n","print(test_set.shape())\n","\n","# Initializing DataLoaders with concise batch_size and shuffle parameters\n","train_loader = DataLoader(train_set, batch_size=100, shuffle=True)\n","val_loader = DataLoader(val_set, batch_size=100, shuffle=True)\n","test_loader = DataLoader(test_set, batch_size=50, shuffle=True)"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"BB_cdvh2KudS","executionInfo":{"status":"ok","timestamp":1710457684910,"user_tz":420,"elapsed":12,"user":{"displayName":"BAICHUAN WANG","userId":"06992025179668429938"}}},"outputs":[],"source":["def train_evaluate(model, train_loader, val_loader, criterion, optimizer, save_path, num_epochs=100):\n","    best_val_accuracy = 0.0  # Initialize best validation accuracy\n","    for epoch in range(num_epochs):\n","        model.train()\n","        running_loss = 0.0\n","        train_correct = 0\n","        train_total = 0\n","        for inputs, labels in train_loader:\n","            inputs, labels = inputs.to(device), labels.to(device)\n","            optimizer.zero_grad()\n","            outputs = model(inputs)\n","            loss = criterion(outputs, labels)\n","            loss.backward()\n","            optimizer.step()\n","            running_loss += loss.item()\n","            _, predicted = torch.max(outputs.data, 1)\n","            train_total += labels.size(0)\n","            train_correct += (predicted == labels).sum().item()\n","\n","        train_accuracy = 100 * train_correct / train_total\n","\n","        model.eval()\n","        val_loss = 0.0\n","        correct = 0\n","        total = 0\n","        with torch.no_grad():\n","            for inputs, labels in val_loader:\n","                inputs, labels = inputs.to(device), labels.to(device)\n","                outputs = model(inputs)\n","                val_loss += loss.item()\n","                _, predicted = torch.max(outputs.data, 1)\n","                total += labels.size(0)\n","                correct += (predicted == labels).sum().item()\n","\n","        val_accuracy = 100 * correct / total\n","        if val_accuracy > best_val_accuracy:\n","            best_val_accuracy = val_accuracy\n","            torch.save(model.state_dict(), save_path)\n","\n","        print(f\"Epoch {epoch+1}/{num_epochs}, Training Accuracy: {train_accuracy}%, Validation Accuracy: {val_accuracy}%, Best Validation Accuracy: {best_val_accuracy}%\")\n","\n","    return best_val_accuracy"]},{"cell_type":"code","source":["# CNN-LSTM hyperparameter grids searchs\n","# takes too long\n","'''\n","learning_rates = [0.0001, 0.001, 0.01, 0.1]\n","batch_sizes = [16, 32, 64, 100]\n","dropout_rates = [0.2, 0.3, 0.4, 0.5, 0.6]\n","filter_sizes = [3, 5, 7]\n","\n","# Placeholder for best hyperparameters and their corresponding validation loss\n","best_params = {}\n","best_val_loss = float('inf')\n","\n","for lr in learning_rates:\n","    for batch_size in batch_sizes:\n","      for dropout_rate in dropout_rates:\n","        for filter_size in filter_sizes:\n","          # Create data loaders with the current batch size\n","          train_loader = DataLoader(train_set, batch_size=100, shuffle=True)\n","          val_loader = DataLoader(val_set, batch_size=100, shuffle=True)\n","\n","          # Initialize model, loss function, and optimizer\n","          model = CNNLSTM(dropout_rate, filter_size)\n","          criterion = nn.CrossEntropyLoss()\n","          optimizer = optim.Adam(model.parameters(), lr=lr)\n","          model = model.to(device)\n","\n","          # Train and evaluate the model\n","          val_loss = train_evaluate(model, train_loader, val_loader, criterion, optimizer, save_path='/content/drive/My Drive/c147-project/models/best_cnn_lstm.pth')\n","          print(f\"LR: {lr}, Batch Size: {batch_size}, Dropout Rate: {dropout_rate}, Filter Size: {filter_size}, Validation Loss: {val_loss}\")\n","\n","          # Update best parameters if current model is better\n","          if val_loss < best_val_loss:\n","            best_val_loss = val_loss\n","            best_params = {'learning_rate': lr, 'batch_size': batch_size, 'dropout_rate': dropout_rate, 'filter_size': filter_size}\n","\n","print(f\"Best Parameters: {best_params}, Best Validation Loss: {best_val_loss}\")\n","'''"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":88},"id":"9OY7pR12Yuh3","outputId":"c69f33b1-739b-44a8-863a-f1059140af9c","executionInfo":{"status":"ok","timestamp":1710457684910,"user_tz":420,"elapsed":10,"user":{"displayName":"BAICHUAN WANG","userId":"06992025179668429938"}}},"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\nlearning_rates = [0.0001, 0.001, 0.01, 0.1]\\nbatch_sizes = [16, 32, 64, 100]\\ndropout_rates = [0.2, 0.3, 0.4, 0.5, 0.6]\\nfilter_sizes = [3, 5, 7]\\n\\n# Placeholder for best hyperparameters and their corresponding validation loss\\nbest_params = {}\\nbest_val_loss = float(\\'inf\\')\\n\\nfor lr in learning_rates:\\n    for batch_size in batch_sizes:\\n      for dropout_rate in dropout_rates:\\n        for filter_size in filter_sizes:\\n          # Create data loaders with the current batch size\\n          train_loader = DataLoader(train_set, batch_size=100, shuffle=True)\\n          val_loader = DataLoader(val_set, batch_size=100, shuffle=True)\\n\\n          # Initialize model, loss function, and optimizer\\n          model = CNNLSTM(dropout_rate, filter_size)\\n          criterion = nn.CrossEntropyLoss()\\n          optimizer = optim.Adam(model.parameters(), lr=lr)\\n          model = model.to(device)\\n\\n          # Train and evaluate the model\\n          val_loss = train_evaluate(model, train_loader, val_loader, criterion, optimizer, save_path=\\'/content/drive/My Drive/c147-project/models/best_cnn_lstm.pth\\')\\n          print(f\"LR: {lr}, Batch Size: {batch_size}, Dropout Rate: {dropout_rate}, Filter Size: {filter_size}, Validation Loss: {val_loss}\")\\n\\n          # Update best parameters if current model is better\\n          if val_loss < best_val_loss:\\n            best_val_loss = val_loss\\n            best_params = {\\'learning_rate\\': lr, \\'batch_size\\': batch_size, \\'dropout_rate\\': dropout_rate, \\'filter_size\\': filter_size}\\n\\nprint(f\"Best Parameters: {best_params}, Best Validation Loss: {best_val_loss}\")\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":7}]},{"cell_type":"code","execution_count":8,"metadata":{"id":"gHRLHbGh-ZO7","executionInfo":{"status":"ok","timestamp":1710457684910,"user_tz":420,"elapsed":9,"user":{"displayName":"BAICHUAN WANG","userId":"06992025179668429938"}},"colab":{"base_uri":"https://localhost:8080/","height":71},"outputId":"25966d9a-f92b-4191-fa69-3e42fa476561"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"\\nbest_params = {'learning_rate': 0.0001, 'batch_size': 75, 'dropout_rate': 0.4, 'filter_size': 7}\\nbest_model = CNNLSTM(dropout_rate=best_params['dropout_rate'], filter_size=best_params['filter_size']).to(device)\\ncriterion = nn.CrossEntropyLoss()\\noptimizer = optim.Adam(best_model.parameters(), lr=best_params['learning_rate'])\\ntrain_loader = DataLoader(train_set, batch_size=best_params['batch_size'], shuffle=True)\\nval_loader = DataLoader(val_set, batch_size=best_params['batch_size'], shuffle=True)\\ntrain_evaluate(best_model, train_loader, val_loader, criterion, optimizer, num_epochs=300, save_path='/content/drive/My Drive/c147-project/models/best_cnn_lstm.pth')\\n\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":8}],"source":["'''\n","best_params = {'learning_rate': 0.0001, 'batch_size': 75, 'dropout_rate': 0.4, 'filter_size': 7}\n","best_model = CNNLSTM(dropout_rate=best_params['dropout_rate'], filter_size=best_params['filter_size']).to(device)\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(best_model.parameters(), lr=best_params['learning_rate'])\n","train_loader = DataLoader(train_set, batch_size=best_params['batch_size'], shuffle=True)\n","val_loader = DataLoader(val_set, batch_size=best_params['batch_size'], shuffle=True)\n","train_evaluate(best_model, train_loader, val_loader, criterion, optimizer, num_epochs=300, save_path='/content/drive/My Drive/c147-project/models/best_cnn_lstm.pth')\n","'''"]},{"cell_type":"code","source":["# Testing loop\n","test_loader = DataLoader(test_set, batch_size=100, shuffle=True)\n","test_best_model = CNNLSTM(0.4, 7).to(device)\n","test_best_model.load_state_dict(torch.load(r'/content/drive/My Drive/c147-project/models/cnn_lstm_7009.pth', map_location=device))\n","test_best_model.eval()\n","correct = 0\n","total = 0\n","with torch.no_grad():\n","    for inputs, labels in test_loader:\n","        inputs, labels = inputs.to(device), labels.to(device)\n","        outputs = test_best_model(inputs)\n","        _, predicted = torch.max(outputs.data, 1)\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item()\n","\n","accuracy = 100 * correct / total\n","print(f'Accuracy of the model on the test set: {accuracy}%')"],"metadata":{"id":"GzGRpIt9svFi","executionInfo":{"status":"ok","timestamp":1710457685444,"user_tz":420,"elapsed":542,"user":{"displayName":"BAICHUAN WANG","userId":"06992025179668429938"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"f5acf096-cefd-4b75-af2b-462becd7f824"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.4 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n"]},{"output_type":"stream","name":"stdout","text":["Accuracy of the model on the test set: 70.03386004514672%\n"]}]}],"metadata":{"accelerator":"GPU","colab":{"machine_shape":"hm","provenance":[],"gpuType":"V100"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.9.undefined"}},"nbformat":4,"nbformat_minor":0}