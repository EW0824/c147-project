{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2824,"status":"ok","timestamp":1710457444489,"user":{"displayName":"BAICHUAN WANG","userId":"06992025179668429938"},"user_tz":420},"id":"_Tkq0x8vDr9j","outputId":"90f8e0a4-4ae0-4de8-f1ef-cdf1ca6cb062"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["# For Google Colab\n","from google.colab import drive\n","drive.mount('/content/drive')\n","import sys\n","import os\n","sys.path.append('/content/drive/My Drive/c147-project')"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"8POg3lKjABRo","executionInfo":{"status":"ok","timestamp":1710457446025,"user_tz":420,"elapsed":1544,"user":{"displayName":"BAICHUAN WANG","userId":"06992025179668429938"}}},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","from sklearn.model_selection import train_test_split\n","\n","from models.cnn import *\n","from scripts.preprocessing import *\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1710457446025,"user":{"displayName":"BAICHUAN WANG","userId":"06992025179668429938"},"user_tz":420},"id":"IdqLdY88ABRp","outputId":"9006c36d-108a-4946-8000-c802d4aa3555"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/My Drive/c147-project/project_data/project\n"]}],"source":["# Load Data\n","%cd /content/drive/My Drive/c147-project/project_data/project\n","X_test = np.load(\"X_test.npy\")\n","y_test = np.load(\"y_test.npy\")\n","person_train_valid = np.load(\"person_train_valid.npy\")\n","X_train_valid = np.load(\"X_train_valid.npy\")\n","y_train_valid = np.load(\"y_train_valid.npy\")\n","person_test = np.load(\"person_test.npy\")\n","\n","## Adjusting the labels so that\n","\n","# Cue onset left - 0\n","# Cue onset right - 1\n","# Cue onset foot - 2\n","# Cue onset tongue - 3\n","\n","y_train_valid -= 769\n","y_test -= 769"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"PV26nQ6RNjMh","executionInfo":{"status":"ok","timestamp":1710457446025,"user_tz":420,"elapsed":2,"user":{"displayName":"BAICHUAN WANG","userId":"06992025179668429938"}}},"outputs":[],"source":["## Data Augumentation\n","def aug(X_train, y_train, X_test, y_test, Trim, sub_sample, average, noise, noid):\n","\n","    total_X_train = None\n","    total_y_train = None\n","    total_X_test = None\n","    total_y_test = None\n","\n","    #Trimming\n","    H = int(Trim * X_train.shape[2])\n","\n","    X_train = X_train[:, :, 0:H]\n","\n","    X_test = X_test[:,:, 0:H]\n","\n","    #Maxpooling\n","    X_max_train = np.max(X_train.reshape(X_train.shape[0], X_train.shape[1], -1, sub_sample), axis=3)\n","\n","    X_max_test = np.max(X_test.reshape(X_test.shape[0], X_test.shape[1], -1, sub_sample), axis=3)\n","\n","    total_X_train = X_max_train\n","    total_y_train = y_train\n","\n","    total_X_test = X_max_test\n","    total_y_test = y_test\n","\n","    #Jittering\n","    X_average_train = np.mean(X_train.reshape(X_train.shape[0], X_train.shape[1], -1, average),axis=3)\n","    X_average_test = np.mean(X_test.reshape(X_test.shape[0], X_test.shape[1], -1, average),axis=3)\n","\n","    X_average_train = X_average_train + np.random.normal(0.0, noise, X_average_train.shape)\n","    X_average_test = X_average_test + np.random.normal(0.0, noise, X_average_test.shape)\n","\n","    total_X_train = np.vstack((total_X_train, X_average_train))\n","    total_y_train = np.hstack((total_y_train, y_train))\n","\n","    total_X_test = np.vstack((total_X_test, X_average_test))\n","    total_y_test = np.hstack((total_y_test, y_test))\n","\n","    #Subsampling\n","    for i in range(sub_sample):\n","\n","        X_subsample_train = X_train[:, :, i::sub_sample] + \\\n","                            (np.random.normal(0.0, noise, X_train[:, :,i::sub_sample].shape) if noid else 0.0)\n","\n","        total_X_train = np.vstack((total_X_train, X_subsample_train))\n","        total_y_train = np.hstack((total_y_train, y_train))\n","\n","        X_subsample_test = X_test[:, :, i::sub_sample] + \\\n","                            (np.random.normal(0.0, noise, X_test[:, :,i::sub_sample].shape) if noid else 0.0)\n","\n","        total_X_test = np.vstack((total_X_test, X_subsample_test))\n","        total_y_test = np.hstack((total_y_test, y_test))\n","\n","\n","    return total_X_train,total_y_train,total_X_test,total_y_test\n","\n","\n","def aug_valid(X_train, y_train, Trim, sub_sample, average, noise, noid):\n","\n","    total_X_train = None\n","    total_y_train = None\n","\n","    #Trimming\n","    H = int(Trim * X_train.shape[2])\n","    X_train = X_train[:, :, 0:H]\n","\n","    #Maxpooling\n","    X_max_train = np.max(X_train.reshape(X_train.shape[0], X_train.shape[1], -1, sub_sample), axis=3)\n","\n","    total_X_train = X_max_train\n","    total_y_train = y_train\n","\n","    return total_X_train,total_y_train"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4829,"status":"ok","timestamp":1710457450852,"user":{"displayName":"BAICHUAN WANG","userId":"06992025179668429938"},"user_tz":420},"id":"eGMiFuK9Njx_","outputId":"307b092c-e404-4823-a1c1-f3e2b79e7cbf"},"outputs":[{"output_type":"stream","name":"stdout","text":["(6768, 22, 500)\n","(423, 22, 500)\n","(1772, 22, 500)\n","torch.Size([6768, 22, 500])\n","torch.Size([423, 22, 500])\n","torch.Size([1772, 22, 500])\n"]}],"source":["X_train, X_valid, y_train, y_valid = train_test_split(X_train_valid, y_train_valid, test_size=0.2, shuffle=True)\n","X_train_prep, y_train_prep, X_test_prep, y_test_prep = aug(X_train,y_train,X_test,y_test,700,2,2,0.5,1)\n","X_valid_prep, y_valid_prep = aug_valid(X_valid,y_valid,700,2,2,0.5,1)\n","\n","print((X_train_prep).shape)\n","print((X_valid_prep).shape)\n","print((X_test_prep).shape)\n","\n","class EEGDataset(Dataset):\n","    def __init__(self, X, Y):\n","        self.X = torch.FloatTensor(X).unsqueeze(1)\n","        self.X = torch.FloatTensor(X) # for conv1d\n","        # self.X = self.X.permute(0, 3, 2, 1)\n","        self.Y = torch.LongTensor(Y)\n","\n","    def __len__(self):\n","        return len(self.X)\n","\n","    def __getitem__(self, index):\n","        return self.X[index], self.Y[index]\n","\n","    def shape(self):\n","        return self.X.shape\n","\n","# Creating Dataset instances\n","train_set = EEGDataset(X_train_prep, y_train_prep)\n","val_set = EEGDataset(X_valid_prep, y_valid_prep)\n","test_set = EEGDataset(X_test_prep, y_test_prep)\n","\n","print(train_set.shape())\n","print(val_set.shape())\n","print(test_set.shape())\n","\n","# Initializing DataLoaders with concise batch_size and shuffle parameters\n","train_loader = DataLoader(train_set, batch_size=100, shuffle=True)\n","val_loader = DataLoader(val_set, batch_size=100, shuffle=True)\n","test_loader = DataLoader(test_set, batch_size=50, shuffle=True)"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"BB_cdvh2KudS","executionInfo":{"status":"ok","timestamp":1710457450853,"user_tz":420,"elapsed":18,"user":{"displayName":"BAICHUAN WANG","userId":"06992025179668429938"}}},"outputs":[],"source":["def train_evaluate(model, train_loader, val_loader, criterion, optimizer, save_path, num_epochs=100):\n","    best_val_accuracy = 0.0  # Initialize best validation accuracy\n","    for epoch in range(num_epochs):\n","        model.train()\n","        running_loss = 0.0\n","        train_correct = 0\n","        train_total = 0\n","        for inputs, labels in train_loader:\n","            inputs, labels = inputs.to(device), labels.to(device)\n","            optimizer.zero_grad()\n","            outputs = model(inputs)\n","            loss = criterion(outputs, labels)\n","            loss.backward()\n","            optimizer.step()\n","            running_loss += loss.item()\n","            _, predicted = torch.max(outputs.data, 1)\n","            train_total += labels.size(0)\n","            train_correct += (predicted == labels).sum().item()\n","\n","        train_accuracy = 100 * train_correct / train_total\n","\n","        model.eval()\n","        val_loss = 0.0\n","        correct = 0\n","        total = 0\n","        with torch.no_grad():\n","            for inputs, labels in val_loader:\n","                inputs, labels = inputs.to(device), labels.to(device)\n","                outputs = model(inputs)\n","                val_loss += loss.item()\n","                _, predicted = torch.max(outputs.data, 1)\n","                total += labels.size(0)\n","                correct += (predicted == labels).sum().item()\n","\n","        val_accuracy = 100 * correct / total\n","        if val_accuracy > best_val_accuracy:\n","            best_val_accuracy = val_accuracy\n","            torch.save(model.state_dict(), save_path)  # Save the best model\n","\n","        print(f\"Epoch {epoch+1}/{num_epochs}, Training Accuracy: {train_accuracy}%, Validation Accuracy: {val_accuracy}%, Best Validation Accuracy: {best_val_accuracy}%\")\n","\n","    return best_val_accuracy"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"vqCxw8-vLap5","colab":{"base_uri":"https://localhost:8080/","height":88},"executionInfo":{"status":"ok","timestamp":1710457450853,"user_tz":420,"elapsed":17,"user":{"displayName":"BAICHUAN WANG","userId":"06992025179668429938"}},"outputId":"56de489e-e949-4ed3-f313-01d3495a4c2b"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\nlearning_rates = [0.0001, 0.001, 0.01, 0.1]\\nbatch_sizes = [16, 32, 64, 100]\\ndropout_rates = [0.2, 0.3, 0.4, 0.5, 0.6]\\nfilter_sizes = [3, 5, 7]\\n\\n# Placeholder for best hyperparameters and their corresponding validation loss\\nbest_params = {}\\nbest_val_loss = float(\\'inf\\')\\n\\nfor lr in learning_rates:\\n    for batch_size in batch_sizes:\\n      for dropout_rate in dropout_rates:\\n        for filter_size in filter_sizes:\\n          # Create data loaders with the current batch size\\n          train_loader = DataLoader(train_set, batch_size=100, shuffle=True)\\n          val_loader = DataLoader(val_set, batch_size=100, shuffle=True)\\n\\n          # Initialize model, loss function, and optimizer\\n          model = CNN(dropout_rate, filter_size)\\n          criterion = nn.CrossEntropyLoss()\\n          optimizer = optim.Adam(model.parameters(), lr=lr)\\n          model = model.to(device)\\n\\n          # Train and evaluate the model\\n          val_loss = train_evaluate(model, train_loader, val_loader, criterion, optimizer)\\n          print(f\"LR: {lr}, Batch Size: {batch_size}, Dropout Rate: {dropout_rate}, Filter Size: {filter_size}, Validation Loss: {val_loss}\")\\n\\n          # Update best parameters if current model is better\\n          if val_loss < best_val_loss:\\n            best_val_loss = val_loss\\n            best_params = {\\'learning_rate\\': lr, \\'batch_size\\': batch_size, \\'dropout_rate\\': dropout_rate, \\'filter_size\\': filter_size}\\n\\nprint(f\"Best Parameters: {best_params}, Best Validation Loss: {best_val_loss}\")\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":7}],"source":["# grid search for cnn hyperparameters\n","# best_params = {'learning_rate': 0.001, 'batch_size': 64, 'dropout_rate': 0.6, 'filter_size': 5}\n","'''\n","learning_rates = [0.0001, 0.001, 0.01, 0.1]\n","batch_sizes = [16, 32, 64, 100]\n","dropout_rates = [0.2, 0.3, 0.4, 0.5, 0.6]\n","filter_sizes = [3, 5, 7]\n","\n","# Placeholder for best hyperparameters and their corresponding validation loss\n","best_params = {}\n","best_val_loss = float('inf')\n","\n","for lr in learning_rates:\n","    for batch_size in batch_sizes:\n","      for dropout_rate in dropout_rates:\n","        for filter_size in filter_sizes:\n","          # Create data loaders with the current batch size\n","          train_loader = DataLoader(train_set, batch_size=100, shuffle=True)\n","          val_loader = DataLoader(val_set, batch_size=100, shuffle=True)\n","\n","          # Initialize model, loss function, and optimizer\n","          model = CNN(dropout_rate, filter_size)\n","          criterion = nn.CrossEntropyLoss()\n","          optimizer = optim.Adam(model.parameters(), lr=lr)\n","          model = model.to(device)\n","\n","          # Train and evaluate the model\n","          val_loss = train_evaluate(model, train_loader, val_loader, criterion, optimizer)\n","          print(f\"LR: {lr}, Batch Size: {batch_size}, Dropout Rate: {dropout_rate}, Filter Size: {filter_size}, Validation Loss: {val_loss}\")\n","\n","          # Update best parameters if current model is better\n","          if val_loss < best_val_loss:\n","            best_val_loss = val_loss\n","            best_params = {'learning_rate': lr, 'batch_size': batch_size, 'dropout_rate': dropout_rate, 'filter_size': filter_size}\n","\n","print(f\"Best Parameters: {best_params}, Best Validation Loss: {best_val_loss}\")\n","'''"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":71},"id":"gHRLHbGh-ZO7","executionInfo":{"status":"ok","timestamp":1710457450853,"user_tz":420,"elapsed":16,"user":{"displayName":"BAICHUAN WANG","userId":"06992025179668429938"}},"outputId":"69933280-5822-43d2-be76-da72b563d2ed"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"\\nbest_params = {'learning_rate': 0.001, 'batch_size': 64, 'dropout_rate': 0.6, 'filter_size': 5}\\nbest_model = CNN(dropout_rate=best_params['dropout_rate'], filter_size=best_params['filter_size']).to(device)\\ncriterion = nn.CrossEntropyLoss()\\noptimizer = optim.Adam(best_model.parameters(), lr=best_params['learning_rate'])\\ntrain_loader = DataLoader(train_set, batch_size=best_params['batch_size'], shuffle=True)\\nval_loader = DataLoader(val_set, batch_size=best_params['batch_size'], shuffle=True)\\ntrain_evaluate(best_model, train_loader, val_loader, criterion, optimizer, num_epochs=200, save_path='/content/drive/My Drive/c147-project/models/best_cnn.pth')\\n\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":8}],"source":["'''\n","best_params = {'learning_rate': 0.001, 'batch_size': 64, 'dropout_rate': 0.6, 'filter_size': 5}\n","best_model = CNN(dropout_rate=best_params['dropout_rate'], filter_size=best_params['filter_size']).to(device)\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(best_model.parameters(), lr=best_params['learning_rate'])\n","train_loader = DataLoader(train_set, batch_size=best_params['batch_size'], shuffle=True)\n","val_loader = DataLoader(val_set, batch_size=best_params['batch_size'], shuffle=True)\n","train_evaluate(best_model, train_loader, val_loader, criterion, optimizer, num_epochs=200, save_path='/content/drive/My Drive/c147-project/models/best_cnn.pth')\n","'''"]},{"cell_type":"code","source":["# Testing loop\n","test_loader = DataLoader(test_set, batch_size=50, shuffle=True)\n","test_best_model = CNN(dropout_rate=0.6, filter_size=5).to(device)\n","test_best_model.load_state_dict(torch.load(r'/content/drive/My Drive/c147-project/models/cnn_7302.pth', map_location=device))\n","test_best_model.eval()\n","correct = 0\n","total = 0\n","with torch.no_grad():\n","    for inputs, labels in test_loader:\n","        inputs, labels = inputs.to(device), labels.to(device)\n","        outputs = test_best_model(inputs)\n","        _, predicted = torch.max(outputs.data, 1)\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item()\n","\n","accuracy = 100 * correct / total\n","print(f'Accuracy of the model on the test set: {accuracy}%')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GzGRpIt9svFi","executionInfo":{"status":"ok","timestamp":1710457451174,"user_tz":420,"elapsed":337,"user":{"displayName":"BAICHUAN WANG","userId":"06992025179668429938"}},"outputId":"4f525395-d378-4072-a895-526efb430280"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy of the model on the test set: 73.53273137697516%\n"]}]}],"metadata":{"accelerator":"GPU","colab":{"machine_shape":"hm","provenance":[],"gpuType":"V100"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.9.undefined"}},"nbformat":4,"nbformat_minor":0}